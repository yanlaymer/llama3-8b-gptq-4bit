{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama-3 GPTQ Quantization and Evaluation\n",
    "\n",
    "This notebook demonstrates the complete pipeline for quantizing Llama-3 models using GPTQ and evaluating the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages\n!pip install -q transformers accelerate datasets huggingface_hub gptqmodel safetensors"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add package to path\n",
    "sys.path.insert(0, str(Path.cwd().parent.parent))\n",
    "\n",
    "from innova_llama3_gptq import quantize_llama3_gptq, GPTQConfig\n",
    "from innova_llama3_gptq.evals import (\n",
    "    evaluate_perplexity_suite,\n",
    "    measure_inference_latency,\n",
    "    create_results_summary\n",
    ")\n",
    "\n",
    "# Set up logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"  # Change this to your model\n",
    "OUTPUT_DIR = \"artifacts/notebook_gptq\"\n",
    "\n",
    "# Quantization configuration\n",
    "BITS = 4\n",
    "GROUP_SIZE = 128\n",
    "CALIBRATION_SAMPLES = 512\n",
    "\n",
    "# Set HF token if needed\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Quantizing {MODEL_ID} to {BITS}-bit GPTQ...\")\n",
    "\n",
    "quantized_model_path = quantize_llama3_gptq(\n",
    "    model_id=MODEL_ID,\n",
    "    bits=BITS,\n",
    "    group_size=GROUP_SIZE,\n",
    "    desc_act=True,\n",
    "    calib_dataset=\"wikitext2\",\n",
    "    max_calib_samples=CALIBRATION_SAMPLES,\n",
    "    out_dir=OUTPUT_DIR,\n",
    "    use_safetensors=True,\n",
    "    seed=42,\n",
    "    auth_token=HF_TOKEN\n",
    ")\n",
    "\n",
    "print(f\"âœ… Quantization complete! Model saved to: {quantized_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate perplexity\n",
    "print(\"Evaluating perplexity...\")\n",
    "\n",
    "perplexity_results = evaluate_perplexity_suite(\n",
    "    model_path=quantized_model_path,\n",
    "    datasets=[\"wikitext2\"],\n",
    "    max_samples_per_dataset=500\n",
    ")\n",
    "\n",
    "for dataset, metrics in perplexity_results.items():\n",
    "    if \"perplexity\" in metrics:\n",
    "        print(f\"  {dataset}: {metrics['perplexity']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure inference latency\n",
    "print(\"Measuring inference latency...\")\n",
    "\n",
    "latency_results = measure_inference_latency(\n",
    "    model_path=quantized_model_path,\n",
    "    batch_sizes=[1, 4],\n",
    "    sequence_length=512,\n",
    "    num_iterations=5\n",
    ")\n",
    "\n",
    "for batch_key, metrics in latency_results.items():\n",
    "    print(f\"  {batch_key}: {metrics['avg_latency_ms']:.2f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Generate Test Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import AutoTokenizer\nfrom gptqmodel import GPTQModel\n\n# Load quantized model\ntokenizer = AutoTokenizer.from_pretrained(quantized_model_path)\nmodel = GPTQModel.load(\n    quantized_model_path,\n    device_map=\"auto\"\n)\n\n# Test generation\ntest_prompts = [\n    \"The future of artificial intelligence is\",\n    \"Climate change can be addressed by\",\n    \"The meaning of life is\"\n]\n\nfor prompt in test_prompts:\n    print(f\"\\nPrompt: {prompt}\")\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=30,\n        temperature=0.7,\n        do_sample=True,\n        pad_token_id=tokenizer.pad_token_id\n    )\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    print(f\"Response: {response}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.utils import get_hardware_info, get_model_size\n",
    "\n",
    "# Create comprehensive summary\n",
    "summary = create_results_summary(\n",
    "    perplexity_results=perplexity_results,\n",
    "    latency_results=latency_results,\n",
    "    model_info={\n",
    "        \"model_id\": MODEL_ID,\n",
    "        \"quantization\": {\n",
    "            \"bits\": BITS,\n",
    "            \"group_size\": GROUP_SIZE,\n",
    "            \"method\": \"gptq\"\n",
    "        },\n",
    "        \"size\": get_model_size(Path(quantized_model_path))\n",
    "    },\n",
    "    hardware_info=get_hardware_info()\n",
    ")\n",
    "\n",
    "# Display key metrics\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"QUANTIZATION SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if \"summary_metrics\" in summary:\n",
    "    for metric, value in summary[\"summary_metrics\"].items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"{metric}: {value:.3f}\")\n",
    "        else:\n",
    "            print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Export to Hugging Face (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate model card\n",
    "from scripts.export_hf_gptq import build_model_card\n",
    "\n",
    "REPO_ID = \"innova/llama3-8b-instruct-gptq-4bit\"  # Change this\n",
    "\n",
    "model_card = build_model_card(\n",
    "    model_dir=Path(quantized_model_path),\n",
    "    base_model=MODEL_ID,\n",
    "    repo_id=REPO_ID,\n",
    "    results_path=None  # Would point to results directory if available\n",
    ")\n",
    "\n",
    "# Save model card\n",
    "with open(Path(quantized_model_path) / \"README.md\", \"w\") as f:\n",
    "    f.write(model_card)\n",
    "\n",
    "print(f\"Model card generated for {REPO_ID}\")\n",
    "print(\"\\nTo upload to Hugging Face Hub:\")\n",
    "print(f\"1. huggingface-cli login\")\n",
    "print(f\"2. huggingface-cli upload {REPO_ID} {quantized_model_path} . --repo-type model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: FP16 vs GPTQ (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell would compare FP16 baseline with GPTQ if you have baseline results\n",
    "# from innova_llama3_gptq.evals import create_comparison_table\n",
    "# import pandas as pd\n",
    "\n",
    "# baseline_results = {...}  # Load baseline results\n",
    "# quantized_results = summary\n",
    "\n",
    "# comparison_df = create_comparison_table(baseline_results, quantized_results)\n",
    "# display(comparison_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}