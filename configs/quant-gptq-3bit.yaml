# GPTQ 3-bit quantization configuration
model:
  id: "meta-llama/Meta-Llama-3-8B-Instruct"
  trust_remote_code: false
  auth_token: "${HF_TOKEN}"

quantization:
  bits: 3
  group_size: 128
  desc_act: true
  sym: true
  true_sequential: true
  use_cuda_fp16: true
  model_seqlen: 2048
  damp_percent: 0.01

calibration:
  dataset: "wikitext2"
  max_samples: 1024  # More samples for 3-bit to maintain quality
  max_length: 2048
  seed: 42

output:
  dir: "artifacts/gptq/3bit"
  use_safetensors: true
  save_tokenizer: true
  save_generation_config: true

hardware:
  device: "cuda"
  batch_size: 1
  use_triton: false
  use_exllama: false  # Disable for 3-bit
  cache_examples_on_gpu: false